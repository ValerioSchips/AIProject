{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn import model_selection\n",
    "import gzip\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from collections import Counter\n",
    "import os\n",
    "import sys\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "from scipy import sparse\n",
    "import scipy\n",
    "#import spacy\n",
    "from collections import Counter\n",
    "from string import punctuation\n",
    "from copy import deepcopy\n",
    "import bottleneck as bn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_path = \"Set4/\"\n",
    "df_path = pre_path+\"DFs/\"\n",
    "matr_path = pre_path+\"matrix/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Property Encodind Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_c_p = dict()\n",
    "encoded_p_c = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â Encodes a type in a sequencial int\n",
    "def encode_property(property_name, plaintext):\n",
    "  global encoded_c_p, encoded_p_c\n",
    "  code = 0\n",
    "  plaintext_str = str(plaintext)\n",
    "  const_last_given_code = '$%lgc%$'\n",
    "  try:\n",
    "    tmp = encoded_p_c[property_name] #Check if property is in dict\n",
    "    try:\n",
    "      code = tmp[plaintext_str] #Check if category is in dict\n",
    "    except:\n",
    "      encoded_p_c[property_name][const_last_given_code] += 1\n",
    "      encoded_p_c[property_name][plaintext_str] = encoded_p_c[property_name][const_last_given_code]\n",
    "      encoded_c_p[property_name][str(encoded_p_c[property_name][const_last_given_code])] = plaintext_str\n",
    "      code = encoded_p_c[property_name][const_last_given_code]\n",
    "  except:\n",
    "    encoded_c_p[property_name] = dict()\n",
    "    encoded_p_c[property_name] = dict()\n",
    "    encoded_p_c[property_name][const_last_given_code] = 0\n",
    "    encoded_p_c[property_name][plaintext_str] = encoded_p_c[property_name][const_last_given_code]\n",
    "    encoded_c_p[property_name][str(0)] = plaintext_str\n",
    "    code = encoded_p_c[property_name][const_last_given_code]\n",
    "  return code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_property(property_name, code):\n",
    "  global encoded_c_p\n",
    "  tmp = encoded_c_p[property_name] #Check if property is in dict\n",
    "  plaintext = tmp[code] #Check if category is in dict\n",
    "  return plaintext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Json Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read file line-by-line and parse json, returns dataframe\n",
    "def parse_json(filename_gzipped_python_json, read_max=-1):\n",
    "  #read gzipped content\n",
    "  f=gzip.open(filename_gzipped_python_json,'r')\n",
    "  \n",
    "  #parse json\n",
    "  parse_data = []\n",
    "  for line in tqdm(f): #tqdm is for showing progress bar, always good when processing large amounts of data\n",
    "    line = line.decode('utf-8')\n",
    "    line = line.replace('true','True') #difference json/python\n",
    "    line = line.replace('false','False')\n",
    "    parsed_result = eval(line) #load python nested datastructure\n",
    "    parse_data.append(parsed_result)\n",
    "    if read_max !=-1 and len(parse_data) > read_max:\n",
    "      print(f'Break reading after {read_max} records')\n",
    "      break\n",
    "  print(f\"Reading {len(parse_data)} rows.\")\n",
    "\n",
    "  #create dataframe\n",
    "  df= pd.DataFrame.from_dict(parse_data)\n",
    "  return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_books_v1 = parse_json('books.json.gz')\n",
    "df_interactions_v0 = parse_json('interactions.json.gz', read_max= 1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interaction for books not present in Book_DF will be removed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_interactions_v1 = df_interactions_v0.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_interactions_v1 = df_books_v1.merge(df_interactions_v1, how='left', on='book_id')[['book_id','user_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_interactions_v1 = df_interactions_v1.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting only relevant interaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we select only the book which are read more than N times and the user which had read more than M books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_interactions_v1_match = df_interactions_v1.drop_duplicates(subset=[\"user_id\",\"book_id\"])\n",
    "\n",
    "book_size = df_interactions_v1.groupby('book_id', as_index=False).user_id.size()\n",
    "book_size = book_size.rename({'size': 'count_item'}, axis='columns')\n",
    "user_size = df_interactions_v1.groupby('user_id', as_index=False).book_id.size()\n",
    "user_size = user_size.rename({'size': 'count_user'}, axis='columns')\n",
    "\n",
    "df_interactions_v1_match = pd.merge(df_interactions_v1_match, book_size, how='left', on=['book_id'])\n",
    "df_interactions_v1_match = pd.merge(df_interactions_v1_match, user_size, how='left', on=['user_id'])\n",
    "\n",
    "df_interactions_v1_match = df_interactions_v1_match[df_interactions_v1_match['count_item'] > 5]\n",
    "df_interactions_v1_match = df_interactions_v1_match[df_interactions_v1_match['count_user'] > 5]\n",
    "#df_interactions_v1_match = df_interactions_v1_match[['book_id', 'user_id', 'weighted_rating']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_interactions_v1 = df_interactions_v1_match.copy()\n",
    "df_interactions_v1 = df_interactions_v1.sort_values(by=['user_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_interactions_v1 = df_interactions_v1[['book_id', 'user_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_interactions_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_interactions_v1.book_id.nunique())\n",
    "print(df_interactions_v1.user_id.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uuid = df_interactions_v1.user_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(98765)\n",
    "uuid_perm = np.random.permutation(uuid.size)\n",
    "uuid = uuid[uuid_perm]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomly divides the user in train set and user set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_users = uuid.size\n",
    "n_heldout_users = int(n_users*0.30)\n",
    "\n",
    "tr_users = uuid[:(n_users - n_heldout_users)]\n",
    "te_users = uuid[(n_users - n_heldout_users):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = df_interactions_v1.loc[df_interactions_v1['user_id'].isin(tr_users)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buid = train_set.book_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = df_interactions_v1.loc[df_interactions_v1['user_id'].isin(te_users)]\n",
    "test_set = test_set.loc[test_set['book_id'].isin(buid)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test_proportion(data, test_prop=0.2):\n",
    "    data_grouped_by_user = data.groupby('user_id')\n",
    "    tr_list, te_list = list(), list()\n",
    "\n",
    "    np.random.seed(98765)\n",
    "\n",
    "    for i, (_, group) in enumerate(data_grouped_by_user):\n",
    "        n_items_u = len(group)\n",
    "\n",
    "        if n_items_u >= 5:\n",
    "            idx = np.zeros(n_items_u, dtype='bool')\n",
    "            idx[np.random.choice(n_items_u, size=int(test_prop * n_items_u), replace=False).astype('int64')] = True\n",
    "\n",
    "            tr_list.append(group[np.logical_not(idx)])\n",
    "            te_list.append(group[idx])\n",
    "        else:\n",
    "            tr_list.append(group)\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "            print(\"%d users sampled\" % i)\n",
    "            sys.stdout.flush()\n",
    "\n",
    "    data_tr = pd.concat(tr_list)\n",
    "    data_te = pd.concat(te_list)\n",
    "    \n",
    "    return data_tr, data_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_tr, test_set_te = split_train_test_proportion(test_set, 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode DF Property(Creates consecutive int id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = [test_set_tr, test_set_te, train_set]\n",
    "for udf in df_list:\n",
    "    udf['book_id'] = udf.book_id.apply(lambda x: encode_property('book_id', x))\n",
    "    udf['user_id'] = udf.user_id.apply(lambda x: encode_property('user_id', x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((train_set.book_id.max()))\n",
    "print((train_set.user_id.max()))\n",
    "print((test_set_tr.book_id.max()))\n",
    "print((test_set_tr.user_id.max()))\n",
    "print((test_set_te.book_id.max()))\n",
    "print((test_set_te.user_id.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_tr.to_csv(df_path+'test_set_tr.csv', index = False)\n",
    "test_set_te.to_csv(df_path+'test_set_te.csv', index = False)\n",
    "train_set.to_csv(df_path+'train_set.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.read_csv(df_path+'train_set.csv')\n",
    "test_set_tr = pd.read_csv(df_path+'test_set_tr.csv')\n",
    "test_set_te = pd.read_csv(df_path+'test_set_te.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = train_set.groupby('user_id').book_id.apply(list).reset_index()\n",
    "test_set_tr = test_set_tr.groupby('user_id').book_id.apply(list).reset_index()\n",
    "test_set_te = test_set_te.groupby('user_id').book_id.apply(list).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates a matrix from a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sparse_matrix(sessions_df, column='history', shape=None):\n",
    "      #flatten\n",
    "  user_ids = []\n",
    "  item_ids = []\n",
    "  for idx, row in sessions_df.iterrows():\n",
    "    items = row[column]\n",
    "    user = row['user_id']\n",
    "    user_ids.extend([user] * len(items))\n",
    "    item_ids.extend(items)\n",
    "  #create csr matrix\n",
    "  values = np.ones(len(user_ids))\n",
    "  matrix = sparse.csr_matrix((values, (user_ids, item_ids)), shape=shape, dtype=np.int32)\n",
    "  return matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates the pair list and matrix Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_list_feature_pairs(XtX, threshold):\n",
    "  AA= sparse.triu(np.abs(XtX), format='csr').todense()\n",
    "  AA[ np.diag_indices(AA.shape[0]) ]=0.0\n",
    "  ii_pairs = np.where((AA>threshold)==True)\n",
    "  return ii_pairs\n",
    "\n",
    "def create_matrix_Z(ii_pairs, X):\n",
    "  MM = sparse.csr_matrix((len(ii_pairs[0]), X.shape[1]), dtype=np.float32)\n",
    "  MM_lil = sparse.lil_matrix(MM)\n",
    "  MM_lil[np.arange(MM_lil.shape[0]) , ii_pairs[0]   ]=1.0\n",
    "  MM_lil[np.arange(MM_lil.shape[0]) , ii_pairs[1]   ]=1.0\n",
    "  MM = sparse.csr_matrix(MM_lil)\n",
    "  #CCmask = 1-MM.todense() # see Eq. 8 in the paper\n",
    "  CCmask = MM.nonzero()\n",
    "  MM=sparse.csc_matrix(MM.T)\n",
    "  Z=  X @ MM\n",
    "  Z= (Z == 2.0 )\n",
    "  Z=Z*1.0\n",
    "  return [ Z, CCmask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate matrix PP and QQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_PP(XtX, XtXdiag, lambdaBB):\n",
    "    ii_diag=np.diag_indices(XtX.shape[0])\n",
    "    XtX[ii_diag] = XtXdiag+lambdaBB\n",
    "    #XtX = XtX.todense()\n",
    "    #XtX_csc = sparse.csc_matrix(XtX, dtype=np.float64)\n",
    "    PP=np.linalg.inv(XtX.todense())\n",
    "    #PP_sparse = sparse.csr_matrix(PP)\n",
    "    return [PP, XtX]\n",
    "\n",
    "def calculate_QQ(ZtZ, ZtZdiag, lambdaCC, rho):\n",
    "    ii_diag_ZZ=np.diag_indices(ZtZ.shape[0])\n",
    "    ZtZ[ii_diag_ZZ] = ZtZdiag+lambdaCC+rho\n",
    "    #ZtZ = ZtZ.todense()\n",
    "    #ZtZ_csc = sparse.csc_matrix(ZtZ, dtype=np.float64)\n",
    "    QQ=np.linalg.inv(ZtZ.todense())\n",
    "    QQ_sparse = sparse.csr_matrix(QQ)\n",
    "    return [QQ_sparse, ZtZ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Param Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 40\n",
    "threshold, lambdaBB, lambdaCC, rho = 110,  500,  5000, 10000 #79,  500,  5000, 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = create_sparse_matrix(train_set, 'book_id')\n",
    "XtX = sparse.csr_matrix(X.T) @ X\n",
    "XtXdiag= deepcopy( XtX.diagonal())\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XtX[ np.diag_indices(XtX.shape[0]) ]=XtXdiag\n",
    "\n",
    "ii_feature_pairs = create_list_feature_pairs(XtX, threshold)\n",
    "print(\"number of feature-pairs: {}\".format(len(ii_feature_pairs[0])))\n",
    "Z, CCmask = create_matrix_Z(np.array(ii_feature_pairs), X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = sparse.csr_matrix(Z, dtype=np.float64)\n",
    "Zt = sparse.csr_matrix(Z.T, dtype = np.float64)\n",
    "Zt.indptr = Zt.indptr.astype(np.uint64)\n",
    "Zt.indices = Zt.indices.astype(np.uint64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ZtZ = Zt.dot(Z)\n",
    "ZtX = sparse.csr_matrix(Z.T) @ X\n",
    "ZtZdiag=deepcopy(ZtZ.diagonal())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PP, XtX = calculate_PP(XtX, XtXdiag, lambdaBB)\n",
    "QQ, ZtZ = calculate_QQ(ZtZ, ZtZdiag, lambdaCC, rho)\n",
    "PP = np.array(PP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_higher(XtX, XtXdiag, ZtZ, CCmask, ZtX, rho, epochs, QQ, PP):\n",
    "    ii_diag=np.diag_indices(XtX.shape[0])\n",
    "    CC = sparse.csr_matrix((ZtZ.shape[0], XtX.shape[0]),dtype=np.float )\n",
    "    DD = sparse.csr_matrix((ZtZ.shape[0], XtX.shape[0]),dtype=np.float )\n",
    "    UU = sparse.csr_matrix((ZtZ.shape[0], XtX.shape[0]),dtype=np.float ) # is Gamma in paper\n",
    "    #PP_d = PP.todense()\n",
    "    for iter in range(epochs):\n",
    "        print(\"epoch {}\".format(iter))\n",
    "        # learn BB\n",
    "        print('learn BB')\n",
    "        XtX[ii_diag] = XtXdiag\n",
    "        tmp = (XtX-(ZtX.T.todense() @ CC.todense()))\n",
    "        BB = np.matmul(PP, tmp)\n",
    "        gamma = np.array(np.divide(BB.diagonal(), PP.diagonal()))[0]\n",
    "        #gamma = sparse.csr_matrix(BB).diagonal() / PP.diagonal()\n",
    "        BB -= PP * gamma\n",
    "        # learn CC\n",
    "        print('learn CC')\n",
    "        CC = sparse.csr_matrix(QQ.todense() @ ((ZtX - sparse.csr_matrix(ZtX.todense() @ BB) +(rho *(DD-UU))).todense()))\n",
    "        #CC= QQ.dot(ZtX - sparse.csr_matrix(ZtX.todense() @ BB) +(rho *(DD-UU)))\n",
    "        # learn DD\n",
    "        print('learn DD')\n",
    "        CC_temp = CC.copy()\n",
    "        CC_temp = sparse.lil_matrix(CC_temp)\n",
    "        CC_temp[CCmask] = 0\n",
    "        CC_temp = sparse.csr_matrix(CC_temp)\n",
    "        DD =  CC_temp\n",
    "        #DD= np.maximum(0.0, DD) # if you want to enforce non-negative parameters\n",
    "        # learn UU (is Gamma in paper)\n",
    "        print('learn UU')\n",
    "        UU+= CC-DD\n",
    "    return [BB,DD]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BB, CC = train_higher(XtX, XtXdiag, ZtZ, CCmask, ZtX, rho, epochs, QQ, PP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(matr_path + \"CC.npy\", CC.todense())\n",
    "np.save(matr_path + \"BB.npy\", BB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CC = np.load(matr_path + \"CC.npy\")\n",
    "BB = np.load(matr_path + \"BB.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_test = test_set_tr.shape[0]\n",
    "idxlist_test = range(N_test)\n",
    "N_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_tr = create_sparse_matrix(test_set_tr, 'book_id')\n",
    "test_data_te = create_sparse_matrix(test_set_te, 'book_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supp_tr = sparse.csr_matrix((test_data_tr.shape[0], (X.shape[1] - test_data_tr.shape[1])), dtype=int)\n",
    "supp_te = sparse.csr_matrix((test_data_te.shape[0], (X.shape[1] - test_data_te.shape[1])), dtype=int)\n",
    "test_data_tr = sparse.csr_matrix(sparse.hstack([test_data_tr, supp_tr]))\n",
    "test_data_te = sparse.csr_matrix(sparse.hstack([test_data_te, supp_te]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_test_data_tr , _ = create_matrix_Z(ii_feature_pairs, test_data_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_data_tr.shape)\n",
    "print(test_data_te.shape)\n",
    "print(X.shape)\n",
    "print(Z.shape)\n",
    "print(CC.shape)\n",
    "print(BB.shape)\n",
    "print(Z_test_data_tr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Z.shape)\n",
    "print(CC.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NDCG_binary_at_k_batch(X_pred, heldout_batch, k=100):\n",
    "    '''\n",
    "    normalized discounted cumulative gain@k for binary relevance\n",
    "    ASSUMPTIONS: all the 0's in heldout_data indicate 0 relevance\n",
    "    '''\n",
    "    batch_users = X_pred.shape[0]\n",
    "    idx_topk_part = bn.argpartition(-X_pred, k, axis=1)\n",
    "    topk_part = X_pred[np.arange(batch_users)[:, np.newaxis],\n",
    "                        idx_topk_part[:, :k]]\n",
    "    idx_part = np.argsort(-topk_part, axis=1)\n",
    "    # X_pred[np.arange(batch_users)[:, np.newaxis], idx_topk] is the sorted\n",
    "    # topk predicted score\n",
    "    idx_topk = idx_topk_part[np.arange(batch_users)[:, np.newaxis], idx_part]\n",
    "    # build the discount template\n",
    "    tp = 1. / np.log2(np.arange(2, k + 2))\n",
    "\n",
    "    DCG = (heldout_batch[np.arange(batch_users)[:, np.newaxis],\n",
    "                            idx_topk].toarray() * tp).sum(axis=1)\n",
    "    IDCG = np.array([(tp[:min(n, k)]).sum()\n",
    "                        for n in heldout_batch.getnnz(axis=1)])\n",
    "    \n",
    "    NDCG = DCG / IDCG\n",
    "    NDCG[np.isnan(NDCG)] = 0\n",
    "    return NDCG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Recall_at_k_batch(X_pred, heldout_batch, k=100):\n",
    "    batch_users = X_pred.shape[0]\n",
    "    \n",
    "    idx = bn.argpartition(-X_pred, k, axis=1)\n",
    "    X_pred_binary = np.zeros_like(X_pred, dtype=bool)\n",
    "    X_pred_binary[np.arange(batch_users)[:, np.newaxis], idx[:, :k]] = True\n",
    "    print(X_pred_binary.shape)\n",
    "    X_true_binary = (heldout_batch > 0).toarray()\n",
    "    tmp = (np.logical_and(X_true_binary, X_pred_binary).sum(axis=1)).astype(np.float64)\n",
    "    recall = np.divide(tmp, np.minimum(k, X_true_binary.sum(axis=1)))\n",
    "    \n",
    "    recall = np.nan_to_num(recall, nan=0.0)\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest = test_data_tr\n",
    "Ztest = Z_test_data_tr\n",
    "\n",
    "if sparse.isspmatrix(Xtest):\n",
    "        Xtest = Xtest.toarray()\n",
    "        Ztest = Ztest.toarray()\n",
    "Xtest = Xtest.astype('int64')\n",
    "Ztest = Ztest.astype('float64')\n",
    "pred_val = (Xtest @ BB) + (Ztest @ CC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r3_list, r5_list, r20_list, r50_list, r10_list = [], [], [], [], []\n",
    "\n",
    "r20_list = (Recall_at_k_batch(pred_val, test_data_te, k=20))\n",
    "r50_list = (Recall_at_k_batch(pred_val, test_data_te, k=50))\n",
    "r10_list = (Recall_at_k_batch(pred_val, test_data_te, k=10))\n",
    "r3_list = (Recall_at_k_batch(pred_val, test_data_te, k=3))\n",
    "r5_list = (Recall_at_k_batch(pred_val, test_data_te, k=5))\n",
    "\n",
    "print(\"Test Recall@3=%.5f (%.5f)\" % (np.mean(r3_list), np.std(r3_list) / np.sqrt(len(r3_list))))\n",
    "print(\"Test Recall@5=%.5f (%.5f)\" % (np.mean(r5_list), np.std(r5_list) / np.sqrt(len(r5_list))))\n",
    "print(\"Test Recall@10=%.5f (%.5f)\" % (np.mean(r10_list), np.std(r10_list) / np.sqrt(len(r10_list))))\n",
    "print(\"Test Recall@20=%.5f (%.5f)\" % (np.mean(r20_list), np.std(r20_list) / np.sqrt(len(r20_list))))\n",
    "print(\"Test Recall@50=%.5f (%.5f)\" % (np.mean(r50_list), np.std(r50_list) / np.sqrt(len(r50_list))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse.save_npz(matr_path+\"Xtest.npz\", sparse.csr_matrix(Xtest))\n",
    "sparse.save_npz(matr_path+\"Ztest.npz\", sparse.csr_matrix(Ztest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt \n",
    "\n",
    "a = pred_val[np.where(pred_val > -3.0)].flatten()\n",
    "plt.figure(dpi=300)\n",
    "plt.hist(a, bins=500, log=True)\n",
    "plt.title(\"Predicted Values Distribution\") \n",
    "plt.axvline(x=0.0, label='0', c='r', linewidth=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "19649b6d477f04954267d7dfcc1e3219afd53992c8847ec6a09d5cd5145b7914"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
